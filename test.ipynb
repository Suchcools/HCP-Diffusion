{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoencoderKL, UNet2DConditionModel, DDPMScheduler\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_latents = torch.Tensor(40, 64, 64, 64).cuda()\n",
    "timesteps = torch.Tensor([615, 678, 556,  34, 389, 288,  84, 299, 459, 916, 660, 135, 903, 818,\n",
    "        978, 724, 331, 354, 636, 679, 211, 213, 288, 469, 964, 393, 515, 459,\n",
    "         61, 691, 939, 205, 163, 692, 900, 784, 395, 612, 445, 586]).long().cuda()\n",
    "encoder_hidden_states = torch.Tensor(40, 77, 768).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of UNet2DConditionModel were not initialized from the model checkpoint at deepghs/animefull-latest and are newly initialized because the shapes did not match:\n",
      "- conv_in.weight: found shape torch.Size([320, 4, 3, 3]) in the checkpoint and torch.Size([320, 64, 3, 3]) in the model instantiated\n",
      "- conv_out.weight: found shape torch.Size([4, 320, 3, 3]) in the checkpoint and torch.Size([64, 320, 3, 3]) in the model instantiated\n",
      "- conv_out.bias: found shape torch.Size([4]) in the checkpoint and torch.Size([64]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "            'deepghs/animefull-latest', subfolder=\"unet\", low_cpu_mem_usage=False, ignore_mismatched_sizes=True, in_channels=64, out_channels=64\n",
    "        ).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 64, 66, 66])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.nn.Conv2d(in_channels=3, out_channels=64, kernel_size=1, stride=1, padding=1)(torch.Tensor(3,3,64,64)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 1, 64, 64])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet(noisy_latents, timesteps, encoder_hidden_states).sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 64, 64, 64])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ImageAdapter(nn.Module):\n",
    "    def __init__(self, in_channels, adapter_channels):\n",
    "        super(ImageAdapter, self).__init__()\n",
    "        self.adapter = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, adapter_channels, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(adapter_channels, in_channels, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.adapter(x)\n",
    "\n",
    "# Example usage\n",
    "in_channels = 64  # Assuming RGB images, adjust as needed\n",
    "adapter_channels = 64\n",
    "\n",
    "# Create an image adapter\n",
    "image_adapter = ImageAdapter(in_channels, adapter_channels)\n",
    "\n",
    "# Example input tensor (batch size, channels, height, width)\n",
    "input_tensor = torch.randn(4, in_channels, 64, 64)\n",
    "\n",
    "# Apply the image adapter to the input tensor\n",
    "image_adapter(input_tensor).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with z of shape (1, 64, 32, 32) = 65536 dimensions.\n",
      "loaded pretrained LPIPS loss from movqgan/modules/losses/lpips/vgg.pth\n",
      "VQLPIPSWithDiscriminator running with hinge loss.\n",
      "use_ema = True\n"
     ]
    }
   ],
   "source": [
    "from movqgan.util import instantiate_from_config\n",
    "from movqgan import get_movqgan_model\n",
    "from omegaconf import OmegaConf\n",
    "import torch\n",
    "config = OmegaConf.load(f\"./vae/vaegan.yaml\")\n",
    "model = instantiate_from_config(config['model'])# Initialize data loaders\n",
    "ckpt_path = f\"./vae/ckpt/step=13499-model.ckpt\"\n",
    "checkpoint = torch.load(ckpt_path)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "vae = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 768, 512])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae.decode(torch.Tensor(1, 64, 96, 64).cuda(),torch.Tensor(1, 64, 96, 64).cuda())[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "hcpdiff.utils.pipe_hook.HookPipe_T2I"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from hcpdiff.utils.pipe_hook import HookPipe_T2I, HookPipe_I2I, HookPipe_Inpaint\n",
    "from hcpdiff.utils.net_utils import to_cpu, to_cuda, auto_tokenizer, auto_text_encoder\n",
    "import torch\n",
    "pretrained_model = \"stablediffusionapi/anything-v5\"\n",
    "HookPipe_T2I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fd6f8ac992d44c09f32a064ba1de28e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/linjw/anaconda3/envs/innoverse/lib/python3.9/site-packages/transformers/models/clip/feature_extraction_clip.py:28: FutureWarning: The class CLIPFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use CLIPImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "pipeline = HookPipe_T2I\n",
    "te = auto_text_encoder(pretrained_model).from_pretrained(pretrained_model, subfolder=\"text_encoder\")\n",
    "tokenizer = auto_tokenizer(pretrained_model).from_pretrained(pretrained_model, subfolder=\"tokenizer\", use_fast=False)\n",
    "pipe = pipeline.from_pretrained(pretrained_model, safety_checker=None, requires_safety_checker=False,\n",
    "                                text_encoder=te, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 768, 512])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.vae.decode(torch.Tensor(1, 4, 96, 64),return_dict=False)[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]],\n",
       "\n",
       "         [[nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          ...,\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan],\n",
       "          [nan, nan, nan,  ..., nan, nan, nan]]]],\n",
       "       grad_fn=<ConvolutionBackward0>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.vae.decode(torch.Tensor(1, 4, 96, 64),return_dict=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "diffusers.models.vae.DecoderOutput"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pipe.vae.decode(torch.Tensor(1, 4, 96, 64),return_dict=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "diffusers.models.vae.DecoderOutput"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(pipe.vae.decode(torch.Tensor(1, 4, 96, 64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecoderOutput(sample=tensor([[[[-6.0997e-17,  1.4013e-45, -5.3636e+03,  ...,  1.5588e-01,\n",
       "            1.5588e-01,  1.5588e-01],\n",
       "          [ 1.5588e-01,  1.5588e-01,         nan,  ...,  1.5588e-01,\n",
       "                   nan,  1.5588e-01],\n",
       "          [ 1.5588e-01,  1.5588e-01,  1.5588e-01,  ...,  1.5588e-01,\n",
       "            1.5588e-01,  1.5588e-01],\n",
       "          ...,\n",
       "          [ 1.5588e-01,  1.5588e-01,  1.5588e-01,  ...,  1.5588e-01,\n",
       "            1.5588e-01,  1.5588e-01],\n",
       "          [ 1.5588e-01,  1.5588e-01,  1.5588e-01,  ...,  1.5588e-01,\n",
       "            1.5588e-01,  1.5588e-01],\n",
       "          [ 1.5588e-01,  1.5588e-01,  1.5588e-01,  ...,  1.5588e-01,\n",
       "            1.5588e-01,  1.5588e-01]],\n",
       "\n",
       "         [[ 9.3767e+02, -8.2520e-02, -8.2520e-02,  ..., -8.2520e-02,\n",
       "           -8.2520e-02, -8.2520e-02],\n",
       "          [-8.2520e-02, -8.2520e-02,         nan,  ..., -8.2520e-02,\n",
       "                   nan, -8.2520e-02],\n",
       "          [-8.2520e-02, -8.2520e-02, -8.2520e-02,  ..., -8.2520e-02,\n",
       "           -8.2520e-02, -8.2520e-02],\n",
       "          ...,\n",
       "          [-8.2520e-02, -8.2520e-02, -8.2520e-02,  ..., -8.2520e-02,\n",
       "           -8.2520e-02, -8.2520e-02],\n",
       "          [-8.2520e-02, -8.2520e-02, -8.2520e-02,  ..., -8.2520e-02,\n",
       "           -8.2520e-02, -8.2520e-02],\n",
       "          [-8.2520e-02, -8.2520e-02, -8.2520e-02,  ..., -8.2520e-02,\n",
       "           -8.2520e-02, -8.2520e-02]],\n",
       "\n",
       "         [[ 1.4628e+03, -1.1127e-01, -1.1127e-01,  ..., -1.1127e-01,\n",
       "           -1.1127e-01, -1.1127e-01],\n",
       "          [-1.1127e-01, -1.1127e-01,         nan,  ..., -1.1127e-01,\n",
       "                   nan, -1.1127e-01],\n",
       "          [-1.1127e-01, -1.1127e-01, -1.1127e-01,  ..., -1.1127e-01,\n",
       "           -1.1127e-01, -1.1127e-01],\n",
       "          ...,\n",
       "          [-1.1127e-01, -1.1127e-01, -1.1127e-01,  ..., -1.1127e-01,\n",
       "           -1.1127e-01, -1.1127e-01],\n",
       "          [-1.1127e-01, -1.1127e-01, -1.1127e-01,  ..., -1.1127e-01,\n",
       "           -1.1127e-01, -1.1127e-01],\n",
       "          [-1.1127e-01, -1.1127e-01, -1.1127e-01,  ..., -1.1127e-01,\n",
       "           -1.1127e-01, -1.1127e-01]],\n",
       "\n",
       "         [[-1.4354e+03,  4.8920e-02,  4.8920e-02,  ...,  4.8920e-02,\n",
       "            4.8920e-02,  4.8920e-02],\n",
       "          [ 4.8920e-02,  4.8920e-02,         nan,  ...,  4.8920e-02,\n",
       "                   nan,  4.8920e-02],\n",
       "          [ 4.8920e-02,  4.8920e-02,  4.8920e-02,  ...,  4.8920e-02,\n",
       "            4.8920e-02,  4.8920e-02],\n",
       "          ...,\n",
       "          [ 4.8920e-02,  4.8920e-02,  4.8920e-02,  ...,  4.8920e-02,\n",
       "            4.8920e-02,  4.8920e-02],\n",
       "          [ 4.8920e-02,  4.8920e-02,  4.8920e-02,  ...,  4.8920e-02,\n",
       "            4.8920e-02,  4.8920e-02],\n",
       "          [ 4.8920e-02,  4.8920e-02,  4.8920e-02,  ...,  4.8920e-02,\n",
       "            4.8920e-02,  4.8920e-02]]]]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import diffusers\n",
    "diffusers.models.vae.DecoderOutput(sample = torch.Tensor(1, 4, 96, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "innoverse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
