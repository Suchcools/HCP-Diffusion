{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import AutoencoderKL, UNet2DConditionModel, DDPMScheduler\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "noisy_latents = torch.Tensor(40, 64, 64, 64).cuda()\n",
    "timesteps = torch.Tensor([615, 678, 556,  34, 389, 288,  84, 299, 459, 916, 660, 135, 903, 818,\n",
    "        978, 724, 331, 354, 636, 679, 211, 213, 288, 469, 964, 393, 515, 459,\n",
    "         61, 691, 939, 205, 163, 692, 900, 784, 395, 612, 445, 586]).long().cuda()\n",
    "encoder_hidden_states = torch.Tensor(40, 77, 768).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of UNet2DConditionModel were not initialized from the model checkpoint at deepghs/animefull-latest and are newly initialized because the shapes did not match:\n",
      "- conv_in.weight: found shape torch.Size([320, 4, 3, 3]) in the checkpoint and torch.Size([320, 64, 3, 3]) in the model instantiated\n",
      "- conv_out.weight: found shape torch.Size([4, 320, 3, 3]) in the checkpoint and torch.Size([1, 320, 3, 3]) in the model instantiated\n",
      "- conv_out.bias: found shape torch.Size([4]) in the checkpoint and torch.Size([1]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "unet = UNet2DConditionModel.from_pretrained(\n",
    "            'deepghs/animefull-latest', subfolder=\"unet\", in_channels=64, out_channels = 1, low_cpu_mem_usage=False, ignore_mismatched_sizes=True\n",
    "        ).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([40, 1, 64, 64])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet(noisy_latents, timesteps, encoder_hidden_states).sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 64, 64, 64])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ImageAdapter(nn.Module):\n",
    "    def __init__(self, in_channels, adapter_channels):\n",
    "        super(ImageAdapter, self).__init__()\n",
    "        self.adapter = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, adapter_channels, kernel_size=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(adapter_channels, in_channels, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.adapter(x)\n",
    "\n",
    "# Example usage\n",
    "in_channels = 64  # Assuming RGB images, adjust as needed\n",
    "adapter_channels = 64\n",
    "\n",
    "# Create an image adapter\n",
    "image_adapter = ImageAdapter(in_channels, adapter_channels)\n",
    "\n",
    "# Example input tensor (batch size, channels, height, width)\n",
    "input_tensor = torch.randn(4, in_channels, 64, 64)\n",
    "\n",
    "# Apply the image adapter to the input tensor\n",
    "image_adapter(input_tensor).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with z of shape (1, 64, 32, 32) = 65536 dimensions.\n",
      "loaded pretrained LPIPS loss from movqgan/modules/losses/lpips/vgg.pth\n",
      "VQLPIPSWithDiscriminator running with hinge loss.\n",
      "use_ema = True\n"
     ]
    }
   ],
   "source": [
    "from movqgan.util import instantiate_from_config\n",
    "from movqgan import get_movqgan_model\n",
    "from omegaconf import OmegaConf\n",
    "import torch\n",
    "config = OmegaConf.load(f\"./vae/vaegan.yaml\")\n",
    "model = instantiate_from_config(config['model'])# Initialize data loaders\n",
    "ckpt_path = f\"./vae/ckpt/step=13499-model.ckpt\"\n",
    "checkpoint = torch.load(ckpt_path)\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "vae = model.cuda()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "innoverse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
