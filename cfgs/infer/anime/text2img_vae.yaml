_base_: 
  - cfgs/infer/text2img.yaml

pretrained_model: 'Lykon/DreamShaper'  # better generic anime model

# safe prompt
prompt: 'masterpiece, best quality, highres, 1girl, solo'

# negative prompt
neg_prompt: 'lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry, white border'
N_repeats: 1  # if prompt or neg_prompt is too long, increase this number
clip_skip: 1

bs: 2
num: 1

# when seed is not set, random seed will be used
# seed: 758691538  # seed for safe
# seed: 465191133  # seed for r18

infer_args:
  width: 512
  height: 768  # image size
  guidance_scale: 7.5  # scale, when higher, the images will tend to be more similar
  num_inference_steps: 30  # how many steps

exp_dir: 'exps/2023-07-26-01-05-35'  # experiment directory
model_steps: 1000  # steps of selected model
emb_dir: '${exp_dir}/ckpts/'
output_dir: 'output/'

merge:

  plugin_cfg:
    InputAdapter:
      _target_: hcpdiff.models.adapter.InputAdapterPatch
      _partial_: True
      lr: 1e-4
      vae_channel: 64
      layers:
        - 'conv_in'
    OutputAdapter:
      _target_: hcpdiff.models.adapter.OutputAdapterPatch
      _partial_: True
      lr: 1e-4
      vae_channel: 64
      layers:
        - 'conv_out'

  group_in:
    type: 'unet'
    base_model_alpha: 0 # base model weight to merge with lora or part
    plugin:
      InputAdapter:
        path: '${.....exp_dir}/ckpts/unet-InputAdapter-${.....model_steps}.safetensors'
      OutputAdapter:
        path: '${.....exp_dir}/ckpts/unet-OutputAdapter-${.....model_steps}.safetensors'

interface:
  - _target_: hcpdiff.vis.DiskInterface
    show_steps: 0
    save_root: '${output_dir}'
